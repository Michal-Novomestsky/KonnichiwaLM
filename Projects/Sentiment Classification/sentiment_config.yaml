transformer_params:
  d_model: 250
  encoder_layers: 5
  num_attention_heads: 5
  d_ff: 250
  dropout: 0.1
  embedding:
    max_positional_embeddings: 50 # Maximum length of tokens which are considered in the positional embedding (i.e. the transformer's "memory")
    epsilon: 1e-12
    vocab_size: 30522 # The size of the tokeniser's vocab

tokeniser_params:
  type: distilbert-base-uncased